{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain_community\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.schema import OutputParserException\n",
    "import json\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Advanced RAG & LangChain Fundamentals\n",
    "\n",
    "This notebook explores:\n",
    "1. **Few-Shot Prompting**\n",
    "2. **Re-Ranking with Cross-Encoders**\n",
    "3. **Conversational Retrieval with Memory**\n",
    "4. **RAG Evaluation**\n",
    "5. **Output Parsing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(title, content):\n",
    "    \"\"\"Helper function to pretty-print results in a more readable format.\"\"\"\n",
    "    print(f\"=== {title} ===\")\n",
    "    print(content)\n",
    "    print(\"================\\n\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key,model_name=\"gpt-4o\")  # Low temperature for deterministic output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Few-Shot Prompting\n",
    "\n",
    "Few-Shot Prompting helps demonstrate a task to the LLM using a small set of examples.  \n",
    "Below, we create a function that:\n",
    "1. Builds a prompt from a list of Q&A examples.\n",
    "2. Asks **you** for a question, then appends it to the final prompt.\n",
    "3. Sends the prompt to the LLM (OpenAI by default), returning the model’s response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_prompt_example():\n",
    "    \"\"\"\n",
    "    This function demonstrates how to build a few-shot prompt.\n",
    "    We'll supply multiple examples to 'teach' the model how to respond\n",
    "    before asking a final question.\n",
    "    \"\"\"\n",
    "    # Example QA pairs\n",
    "    examples = [\n",
    "        {\n",
    "            \"input\": \"What is the capital of France?\",\n",
    "            \"output\": \"Paris.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"What is the capital of Spain?\",\n",
    "            \"output\": \"Madrid.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Whats the capital of Italy?\",\n",
    "            \"output\": \"Rome.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Whats denmarks capital city?\",\n",
    "            \"output\": \"Copenhagen.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Build a string that contains the few-shot examples\n",
    "    example_prompt = \"\"\n",
    "    for ex in examples:\n",
    "        example_prompt += f\"Q: {ex['input']}\\nA: {ex['output']}\\n\\n\"\n",
    "\n",
    "    # Create a template with placeholders\n",
    "    template = (\n",
    "        \"Use the examples below to answer the question.\\n\\n\"\n",
    "        \"{examples}\\n\"\n",
    "        \"Q: {question}\\nA:\"\n",
    "    )\n",
    "\n",
    "    # Prompt the user to enter their own question\n",
    "    user_question = input(\"Enter a question ('What is the capital of ___?'): \")\n",
    "    \n",
    "    filled_prompt = template.format(examples=example_prompt, question=user_question)\n",
    "    \n",
    "    response = llm.invoke(filled_prompt)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "response_few_shot = few_shot_prompt_example()\n",
    "pretty_print(\"Few-Shot Prompt Response\", response_few_shot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 2: Re-Ranking with a Cross-Encoder\n",
    "**Why Re-Rank?**  - Approximate nearest neighbor (ANN) searches may return decent but not always perfectly ranked results.\n",
    "\n",
    "**Cross-Encoder** re-evaluates (query, document) pairs to produce a more accurate relevance score.\n",
    "**Steps**:\n",
    "1. Create a small set of dummy documents.\n",
    "2. Store them in a FAISS vector store with embeddings.\n",
    "3. Retrieve the top-k documents.\n",
    "4. Re-rank those documents using a cross-encoder model.\n",
    "5. Return the highest-scoring document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def advanced_rag_rerank_example():\n",
    "    \"\"\"\n",
    "    1. Create a diverse set of dummy documents\n",
    "    2. Store them in a FAISS vector store using OpenAI embeddings\n",
    "    3. Retrieve the top 5 most relevant results\n",
    "    4. Re-rank them using a cross-encoder model\n",
    "    5. Return the best-ranked document with scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Create a diverse set of dummy documents\n",
    "    texts = [\n",
    "        \"Paris is the capital of France, known for its iconic Eiffel Tower, rich art, and the Louvre Museum.\",\n",
    "        \"Berlin, the capital of Germany, is famous for its history, culture, and landmarks such as the Brandenburg Gate.\",\n",
    "        \"Madrid, the capital of Spain, is a vibrant city known for its Royal Palace and Plaza Mayor.\",\n",
    "        \"The Eiffel Tower, located in Paris, is one of the most visited monuments in the world.\",\n",
    "        \"New York City is known for the Statue of Liberty, Times Square, and as a global financial hub.\",\n",
    "        \"Tokyo is the capital of Japan, a metropolis blending modern skyscrapers with historic temples.\",\n",
    "        \"The Colosseum, an ancient Roman amphitheater, is located in Rome, Italy.\",\n",
    "        \"London, the capital of the United Kingdom, is home to Big Ben, Buckingham Palace, and the River Thames.\",\n",
    "        \"The Seine River runs through Paris, offering scenic views of the city's famous landmarks.\",\n",
    "        \"Barcelona, Spain, is renowned for its stunning architecture, including the Sagrada Familia and Park Güell.\",\n",
    "        \"The Louvre Museum in Paris houses thousands of artworks, including the Mona Lisa.\",\n",
    "        \"The Champs-Élysées in Paris is a famous avenue lined with luxury shops, theaters, and cafés.\",\n",
    "        \"The Golden Gate Bridge in San Francisco is an iconic suspension bridge known for its striking red color.\",\n",
    "        \"The Pyramids of Giza in Egypt are among the world's most famous ancient wonders.\",\n",
    "        \"Sydney, Australia, is known for its Opera House, Harbour Bridge, and beautiful beaches.\",\n",
    "        \"Mount Everest, the tallest mountain in the world, is located on the border between Nepal and Tibet.\"\n",
    "    ]\n",
    "    \n",
    "    docs = [Document(page_content=t) for t in texts]\n",
    "\n",
    "    # Step 2: Initialize FAISS vector store with OpenAI embeddings\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    # Step 3: User inputs a query\n",
    "    user_query = input(\"\\nEnter a query (e.g., Ask about opera houses or fine arts): \")\n",
    "    retrieved_docs = vectorstore.similarity_search(user_query, k=5)\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        print(\"\\nNo relevant documents found.\")\n",
    "        return None, []\n",
    "\n",
    "    # Step 4: Use CrossEncoder instead of SentenceTransformer\n",
    "    model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    # Prepare input pairs\n",
    "    inputs = [(user_query, d.page_content) for d in retrieved_docs]\n",
    "\n",
    "    # Get scores using `model.predict()`\n",
    "    scores = model.predict(inputs)  # Now correctly applies cross-encoder scoring\n",
    "\n",
    "    # Step 5: Sort and return results\n",
    "    ranked_docs = sorted(zip(retrieved_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    best_doc = ranked_docs[0][0]  # Best document based on ranking\n",
    "\n",
    "    # Step 6: Display results\n",
    "    print(\"\\n=== Re-Ranked Results ===\")\n",
    "    for rank, (doc, score) in enumerate(ranked_docs, 1):\n",
    "        print(f\"{rank}. Score: {score:.4f} | Content: {doc.page_content}\")\n",
    "    print(\"=========================\")\n",
    "\n",
    "    return best_doc, ranked_docs\n",
    "\n",
    "# Run the function\n",
    "best_doc, ranked_docs = advanced_rag_rerank_example()\n",
    "\n",
    "# Display the best result separately\n",
    "if best_doc:\n",
    "    print(\"\\n=== Best Retrieved Document ===\")\n",
    "    print(best_doc.page_content)\n",
    "    print(\"==============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Conversational Retrieval with Memory\n",
    "\n",
    "LangChain’s **Memory** component preserves conversation history.  \n",
    "Below, we use a **ConversationBufferMemory** that keeps all prior messages in a buffer.\n",
    "\n",
    "We'll simulate:\n",
    "1. A user greeting.\n",
    "2. AI greeting response.\n",
    "3. Another user question.\n",
    "4. AI response.\n",
    "\n",
    "In a real pipeline, this memory would be fed into a retrieval step so the system can reference the conversation context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
    "\n",
    "# Set up a conversational agent with memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def conversational_retrieval_example():\n",
    "    \"\"\"\n",
    "    Demonstrates how an AI conversation is dynamically managed using LLM and memory.\n",
    "    - AI remembers niche facts that wouldn't be general knowledge.\n",
    "    - User input is processed dynamically.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nAI: Hello! I can remember details from our conversation. Ask me about niche topics.\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
    "            print(\"\\nExiting conversation...\")\n",
    "            break\n",
    "\n",
    "        # Generate response\n",
    "        response = conversation.predict(input=user_input)\n",
    "\n",
    "        print(f\"\\nAI: {response}\")\n",
    "\n",
    "    return memory\n",
    "\n",
    "# Run conversation\n",
    "conv_memory = conversational_retrieval_example()\n",
    "\n",
    "# Display conversation history\n",
    "print(\"\\n=== Conversation History ===\")\n",
    "for msg in conv_memory.chat_memory.messages:\n",
    "    role = \"User\" if msg.type == \"user\" else \"AI\"\n",
    "    print(f\"{role}: {msg.content}\")\n",
    "print(\"============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: RAG Evaluation\n",
    "\n",
    "To measure how well a RAG system performs, we can:\n",
    "- Compare retrieved documents to a “gold standard” set.\n",
    "- Compare final answers to reference answers.\n",
    "\n",
    "**QAEvalChain** (from LangChain) is a convenient helper to assess the correctness of QA outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag_evaluation_example():\n",
    "    \"\"\"\n",
    "    Simulates an evaluation:\n",
    "    - Question: \"Where is the Eiffel Tower located?\"\n",
    "    - Reference Answer: \"The Eiffel Tower is located in Paris.\"\n",
    "    - Predicted Answer: \"It's located in central Paris.\"\n",
    "    \n",
    "    Uses QAEvalChain from LangChain to evaluate the prediction.\n",
    "    \"\"\"\n",
    "    question = \"Where is the Eiffel Tower located?\"\n",
    "    reference_answer = \"The Eiffel Tower is located in Paris.\"\n",
    "    predicted_answer = \"It's located in central Paris.\"\n",
    "    \n",
    "    # Instantiate QAEvalChain\n",
    "    eval_chain = QAEvalChain.from_llm(llm)\n",
    "    \n",
    "    # Prepare example and prediction\n",
    "    example = {\"query\": question, \"answer\": reference_answer}\n",
    "    prediction = {\"query\": question, \"result\": predicted_answer}\n",
    "    \n",
    "    # Evaluate\n",
    "    graded = eval_chain.evaluate(\n",
    "        examples=[example],  # List of reference examples\n",
    "        predictions=[prediction]  # List of model predictions\n",
    "    )\n",
    "    \n",
    "    return graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and print the evaluation result\n",
    "#Try adjusting the values above to see if it correctly classifies\n",
    "eval_result = rag_evaluation_example()\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 5: Output ParsingOften, we need structured output (JSON, CSV, etc.).  \n",
    "\n",
    "**Output Parsing** ensures the LLM's output can be safely converted into a machine-readable format.  Below, we ask the LLM to respond in JSON. If it fails, we raise an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_parsing_example():\n",
    "    \"\"\"\n",
    "    1. User enters a question.\n",
    "    2. LLM generates a JSON-formatted response.\n",
    "    3. Parse the response and return a structured dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Get user input\n",
    "    user_query = input(\"\\nEnter a question for JSON-formatted response (e.g., 'What is the capital of Germany?'): \")\n",
    "\n",
    "    # Step 2: Generate response\n",
    "    prompt = f\"Answer the following question in **valid JSON format only** (without markdown formatting): {{'question': '{user_query}', 'answer': 'your response here'}}\"\n",
    "    response = llm.invoke(prompt)  # FIX: Use `invoke()` instead of `predict()`\n",
    "\n",
    "    # Step 3: Extract text from AIMessage object\n",
    "    response_text = response.content if hasattr(response, \"content\") else str(response)\n",
    "\n",
    "    # Step 4: Remove JSON markdown formatting if present\n",
    "    if response_text.startswith(\"```json\"):\n",
    "        response_text = response_text.strip(\"```json\").strip(\"```\")  # Remove formatting\n",
    "\n",
    "    # Step 5: Parse JSON\n",
    "    try:\n",
    "        parsed_output = json.loads(response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        raise OutputParserException(f\"Failed to parse LLM output as JSON. Raw output: {response_text}\")\n",
    "\n",
    "    return parsed_output\n",
    "\n",
    "# Run the function\n",
    "parsed_output = output_parsing_example()\n",
    "\n",
    "# Print formatted output\n",
    "print(\"\\n=== Parsed JSON Output ===\")\n",
    "print(json.dumps(parsed_output, indent=4))\n",
    "print(\"==========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Next Steps\n",
    "\n",
    "You have now seen:\n",
    "\n",
    "1. **Few-Shot Prompting**  \n",
    "2. **Re-Ranking** with a cross-encoder  \n",
    "3. **Conversational Memory**  \n",
    "4. **RAG Evaluation**  \n",
    "5. **Output Parsing**\n",
    "\n",
    "\n",
    "**Thank you**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
