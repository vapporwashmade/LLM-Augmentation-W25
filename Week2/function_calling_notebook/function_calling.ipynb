{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================================\n",
    "# Jupyter Notebook: Function Calling 101\n",
    "# ====================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scenario:\n",
    "    We have 4 functions:\n",
    "      1. get_weather_info(city)\n",
    "      2. book_flight(loc_origin, loc_destination, datetime, airline)\n",
    "      3. extract_entities\n",
    "      4. tag text\n",
    "\n",
    "We present them to the OpenAI model with 'function_descriptions'.\n",
    "When the user asks a question that matches the function usage, \n",
    "the model will produce a structured function call (JSON) with name & arguments.\n",
    "\n",
    "We'll manually parse the result from the model, call our Python function, \n",
    "and then feed that result back to the model to produce a final user-facing answer.\n",
    "\n",
    "TODO:\n",
    "  - Add more interesting scenarios (like file_complaint).\n",
    "  - Experiment with 'function_call=\"auto\"' vs. forced or required.\n",
    "  - Combine multiple user requests into a single prompt to see if the model calls \n",
    "    multiple functions or just one.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################\n",
    "# Section 1: Setup\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Key Found: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We'll use environment variables to store API keys.\n",
    "Make sure you have an OPENAI_API_KEY environment variable set.\n",
    "\"\"\"\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"OpenAI Key Found:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n",
    "\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 2) Define Python functions to be \"called\"\n",
    "# -------------------------------------------------\n",
    "In this cell, we define the **actual** Python functions the model can “call.”  \n",
    "- `get_weather_info(city)`: Returns mocked weather data (in real usage, you'd call an actual weather API).  \n",
    "- `book_flight(...)`: Pretends to book a flight and returns a JSON-formatted confirmation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_info(city: str):\n",
    "    \"\"\"\n",
    "    Dummy function that returns made-up weather data.\n",
    "    In reality, you'd call a weather API (like OpenWeatherMap).\n",
    "    \"\"\"\n",
    "    fake_data = {\n",
    "        \"Amsterdam\": {\"temp\": 15, \"condition\": \"Drizzle\"},\n",
    "        \"New York\": {\"temp\": 22, \"condition\": \"Sunny\"},\n",
    "        \"Paris\": {\"temp\": 16, \"condition\": \"Overcast\"}\n",
    "    }\n",
    "    weather = fake_data.get(city, {\"temp\": 0, \"condition\": \"Unknown\"})\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"city\": city,\n",
    "        \"temperature_c\": weather[\"temp\"],\n",
    "        \"conditions\": weather[\"condition\"]\n",
    "    })\n",
    "\n",
    "def book_flight(loc_origin: str, loc_destination: str, datetime_str: str, airline: str):\n",
    "    \"\"\"\n",
    "    Dummy function to 'book' a flight.\n",
    "    In reality, you'd integrate with an airline or travel booking API.\n",
    "    \"\"\"\n",
    "    return json.dumps({\n",
    "        \"status\": \"success\",\n",
    "        \"origin\": loc_origin,\n",
    "        \"destination\": loc_destination,\n",
    "        \"datetime\": datetime_str,\n",
    "        \"airline\": airline,\n",
    "        \"confirmation_number\": \"ABC123XYZ\"\n",
    "    })\n",
    "\n",
    "def extract_entities(text: str):\n",
    "    \"\"\"\n",
    "    Dummy function that 'extracts' person names and ages from text.\n",
    "    We'll simulate the result as a simple dictionary.\n",
    "    In reality, you'd do more sophisticated NER or rely on LLM logic directly.\n",
    "    \"\"\"\n",
    "    # Very naive \"parser\"\n",
    "    # If it sees \"Joe is 30\" => we store that as an entity\n",
    "    entities = []\n",
    "    words = text.split()\n",
    "    for i, w in enumerate(words):\n",
    "        if w.lower() in [\"joe\", \"mary\", \"bob\"]:\n",
    "            # check if next words might be \"is <age>\"\n",
    "            if i+2 < len(words) and words[i+1].lower() in [\"is\"] and words[i+2].isdigit():\n",
    "                entities.append({\"name\": w.capitalize(), \"age\": int(words[i+2])})\n",
    "            else:\n",
    "                entities.append({\"name\": w.capitalize(), \"age\": None})\n",
    "    return json.dumps({\"entities\": entities})\n",
    "\n",
    "def tag_text(text: str):\n",
    "    \"\"\"\n",
    "    Dummy function for tagging text with sentiment + language.\n",
    "    In reality, you'd call a sentiment classifier or language detection library.\n",
    "    \"\"\"\n",
    "    # We'll simulate some trivial checks:\n",
    "    sentiment = \"neutral\"\n",
    "    if any(x in text.lower() for x in [\"love\", \"great\", \"amazing\"]):\n",
    "        sentiment = \"pos\"\n",
    "    elif any(x in text.lower() for x in [\"hate\", \"terrible\", \"bad\", \"dislike\"]):\n",
    "        sentiment = \"neg\"\n",
    "\n",
    "    # We'll do a naive language detection check for 'mi piace' => italian\n",
    "    language = \"en\"\n",
    "    if \"mi piace\" in text.lower():\n",
    "        language = \"it\"\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"sentiment\": sentiment,\n",
    "        \"language\": language\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Describe these functions for OpenAI\n",
    "# -------------------------------------------------\n",
    "This cell simply displays all the functions (or “tools”) we’ve defined. We assign each function a name, description, and a JSON schema for its arguments, so the model knows how to call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function descriptions loaded.\n"
     ]
    }
   ],
   "source": [
    "# List of function descriptions for API calls\n",
    "function_descriptions = [\n",
    "    {\n",
    "        \"type\": \"function\",  # Defines this as a function tool\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather_info\",  # Function name used by the LLM\n",
    "            \"description\": \"Retrieve current weather information for a city.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",  # Function expects an object as input\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",  # City name as input\n",
    "                        \"description\": \"City to retrieve weather data for, e.g., 'Amsterdam'.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"city\"]  # City is mandatory\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"book_flight\",\n",
    "            \"description\": \"Book a flight between two locations with a preferred airline.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"loc_origin\": {  \n",
    "                        \"type\": \"string\",  # Departure location\n",
    "                        \"description\": \"3-letter airport code or city name of departure.\"\n",
    "                    },\n",
    "                    \"loc_destination\": {\n",
    "                        \"type\": \"string\",  # Arrival location\n",
    "                        \"description\": \"3-letter airport code or city name of arrival.\"\n",
    "                    },\n",
    "                    \"datetime\": {\n",
    "                        \"type\": \"string\",  # Date/Time in ISO format\n",
    "                        \"description\": \"Flight date/time in ISO format, e.g., '2024-06-01 08:00'.\"\n",
    "                    },\n",
    "                    \"airline\": {\n",
    "                        \"type\": \"string\",  # Airline preference\n",
    "                        \"description\": \"Preferred airline for booking.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"loc_origin\", \"loc_destination\", \"datetime\", \"airline\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"extract_entities\",\n",
    "            \"description\": \"Extract named entities (e.g., person name, age) from text.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"string\",  # Input text containing entities\n",
    "                        \"description\": \"Text to analyze for named entity recognition (NER).\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"text\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"tag_text\",\n",
    "            \"description\": \"Analyze text and tag it with sentiment and language.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"string\",  # Input text to be classified\n",
    "                        \"description\": \"Text to be tagged with sentiment and language classification.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"text\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Function descriptions loaded.\")  # Confirmation message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Define and Document the Function `test_call_model`\n",
    "\n",
    "In this cell, we create a helper function named `test_call_model`. It:\n",
    "1. Accepts a user prompt (`user_message`) and a `function_call` mode (`\"auto\"`, `\"none\"`, or `{\"name\": \"...function_name...\"}`).\n",
    "2. Calls the OpenAI API with our **function (tool) descriptions** so the model knows which functions are available.\n",
    "3. Returns the model's raw response, which may include a function call if it decides that is relevant.\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "\n",
    " -------------------------------------------------\n",
    " Quick test with 'get_weather_info'\n",
    " -------------------------------------------------\n",
    " Purpose: \n",
    " Test if the model calls the \"get_weather_info\" function \n",
    " when we ask about the weather in Amsterdam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We'll pass our function schema to the model. The model can decide\n",
    "to call one function, multiple, or none, depending on user input.\n",
    "\n",
    "- function_call=\"auto\" => The model decides if/when to call.\n",
    "- function_call=\"none\" => The model cannot call any function.\n",
    "- function_call={\"name\":\"book_flight\"} => Force it to call 'book_flight'.\n",
    "\n",
    "Try toggling these below in the 'test_call_model' function.\n",
    "\"\"\"\n",
    "\n",
    "def test_call_model(user_message: str, function_call=\"auto\"):\n",
    "    \"\"\"\n",
    "    1) We send user_message + function_descriptions to the model\n",
    "    2) We see if it returns a function call\n",
    "    3) If so, we parse arguments, call the function ourselves\n",
    "    4) Return final output\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "        tools=function_descriptions,  # 'functions' is now called 'tools'\n",
    "        tool_choice=function_call,    # 'function_call' is now 'tool_choice'\n",
    "    )\n",
    "    response = completion.choices[0].message\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test with `get_weather_info`**\n",
    "Here, we try a simple user prompt asking about the weather in Amsterdam.  \n",
    "- We use `function_call=\"auto\"` so the model may decide to invoke our `get_weather_info` function if it deems it relevant.  \n",
    "- The cell prints out the raw model response so we can see if it includes a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_FgQ0Zk0gxuoiMn1ZEC5E4KcS', function=Function(arguments='{\"city\":\"Los Angeles\"}', name='get_weather_info'), type='function')])\n",
      "\n",
      "We expect a 'function_call' to get_weather_info.\n"
     ]
    }
   ],
   "source": [
    "# Let's do a quick test with something that calls 'get_weather_info'\n",
    "user_prompt_1 = \"What is the weather in Los Angeles?\"\n",
    "resp = test_call_model(user_prompt_1, function_call=\"auto\")\n",
    "print(\"Model response:\\n\", resp)\n",
    "print(\"\\nWe expect a 'function_call' to get_weather_info.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 5) Parse the response and call the function\n",
    "# -------------------------------------------------\n",
    "\n",
    " -------------------------------------------------\n",
    " Why This Code is Useful:\n",
    " Many LLM-based apps need to handle model outputs that\n",
    " specify a \"function call.\" This code acts as a \"bridge\":\n",
    " - If the LLM wants to invoke a function (to fetch data or\n",
    "   perform an action), we parse the model's tool_call info,\n",
    "   call the real Python function, and then use another API\n",
    "   call to produce a final, user-facing answer.\n",
    " - This approach is crucial for letting LLMs integrate\n",
    "   with external systems (e.g., weather APIs, booking\n",
    "   services) in a controlled, structured manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model wants to call function get_weather_info with args {'city': 'Los Angeles'}\n",
      "\n",
      "Final text back to user:\n",
      " It looks like there was an error retrieving the complete weather information for Los Angeles. The temperature is showing as 0°C, and the conditions are listed as unknown. Please check the weather manually or try again later for updated information.\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def handle_function_call(response) -> str:\n",
    "    \"\"\"\n",
    "    If the response contains a function call, execute it, return the result,\n",
    "    and pass it back to the model for a final answer.\n",
    "    \"\"\"\n",
    "    # If there's no function call, return the assistant's response as normal\n",
    "    if response.content and not hasattr(response, \"tool_calls\"):\n",
    "        return response.content\n",
    "\n",
    "    # Extract function call information\n",
    "    tool_calls = response.tool_calls\n",
    "    if not tool_calls:\n",
    "        return response.content\n",
    "\n",
    "    # Process the first function call in the list\n",
    "    tool_call = tool_calls[0]\n",
    "    fn_name = tool_call.function.name\n",
    "    fn_args = json.loads(tool_call.function.arguments)\n",
    "    tool_call_id = tool_call.id  # Extract tool_call_id\n",
    "    print(f\"Model wants to call function {fn_name} with args {fn_args}\")\n",
    "\n",
    "    # Route to the correct function\n",
    "    if fn_name == \"get_weather_info\":\n",
    "        city_req = fn_args[\"city\"]\n",
    "        function_result = get_weather_info(city_req)\n",
    "\n",
    "    elif fn_name == \"book_flight\":\n",
    "        loc_origin = fn_args[\"loc_origin\"]\n",
    "        loc_dest = fn_args[\"loc_destination\"]\n",
    "        dt = fn_args[\"datetime\"]\n",
    "        airline = fn_args[\"airline\"]\n",
    "        function_result = book_flight(loc_origin, loc_dest, dt, airline)\n",
    "    elif fn_name == \"extract_entities\":\n",
    "        text = fn_args[\"text\"]\n",
    "        function_result = extract_entities(text)\n",
    "    elif fn_name == \"tag_text\":\n",
    "        text = fn_args[\"text\"]\n",
    "        function_result = tag_text(text)\n",
    "    else:\n",
    "        return \"Function not recognized\"\n",
    "\n",
    "    # Ensure function_result is a string\n",
    "    function_result = str(function_result)\n",
    "\n",
    "    # Handle possible None content\n",
    "    previous_content = f\"Previously: {response.content}\" if response.content else \"Processing function call...\"\n",
    "\n",
    "    # Make a second model request, providing the function result\n",
    "    # so it can finalize a user-facing response. \n",
    "    # This step is commonly used to incorporate the function's data\n",
    "    # (e.g., \"temp=15C, partly cloudy\") into a full natural language\n",
    "    # answer for the end user.\n",
    "    second_response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": previous_content},  \n",
    "            {\"role\": \"assistant\", \"tool_calls\": response.tool_calls},  # Include the original tool call\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call_id,\n",
    "                \"name\": fn_name,\n",
    "                \"content\": function_result\n",
    "            }  \n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Extract final answer\n",
    "    final_answer = second_response.choices[0].message.content\n",
    "    return final_answer\n",
    "\n",
    "# Test it end-to-end\n",
    "final_text = handle_function_call(resp)\n",
    "print(\"\\nFinal text back to user:\\n\", final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 6) Another Example: Book a flight\n",
    "# -------------------------------------------------\n",
    " Why This Is Useful:\n",
    " Demonstrates how a user request can trigger the \"book_flight\" function.\n",
    " This pattern can generalize to many use cases like \"place an order\",\n",
    " \"schedule a meeting\", etc. The LLM decides the best function to call\n",
    " and we do the behind-the-scenes work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yEIjxICvIMMLIorEJbMcYSBk', function=Function(arguments='{\"loc_origin\":\"SFO\",\"loc_destination\":\"DTW\",\"datetime\":\"2025-02-20T22:00:00\",\"airline\":\"Delta\"}', name='book_flight'), type='function')])\n",
      "Model wants to call function book_flight with args {'loc_origin': 'SFO', 'loc_destination': 'DTW', 'datetime': '2025-02-20T22:00:00', 'airline': 'Delta'}\n",
      "\n",
      "User-Facing Answer:\n",
      " The flight from San Francisco (SFO) to Detroit (DTW) on February 20, 2025, at 22:00 has been successfully booked with Delta Airlines. Your confirmation number is ABC123XYZ.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We'll ask a multi-part question that might trigger the model to call the second function.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_2 = \"I want to book a flight from SFO to DTW on Feb 20, 2025 at 10pm with Delta\"\n",
    "resp2 = test_call_model(user_prompt_2, function_call=\"auto\")\n",
    "print(\"Model response:\\n\", resp2)\n",
    "\n",
    "final_text_2 = handle_function_call(resp2)\n",
    "print(\"\\nUser-Facing Answer:\\n\", final_text_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 7)  Section: Entity Extraction\n",
    "# -------------------------------------------------\n",
    " Why This Is Useful:\n",
    " Shows how the LLM can parse unstructured text (e.g., \"Joe is 30,\n",
    " Mary is older...\") and call a function that extracts relevant entities\n",
    " or structured data. This is a building block for advanced data extraction,\n",
    " knowledge-base population, or record creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Entity Extraction Test ===\n",
      "Model response:\n",
      " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Hvm93p441oSdNTyeJnScp5nQ', function=Function(arguments='{\"text\":\"Joe is 30, Mary is older but we don\\'t know her age.\"}', name='extract_entities'), type='function')])\n",
      "Model wants to call function extract_entities with args {'text': \"Joe is 30, Mary is older but we don't know her age.\"}\n",
      "\n",
      "User-Facing Answer:\n",
      " The text mentions two individuals, Joe and Mary. Joe's age is explicitly mentioned as 30, whereas Mary's age is indicated as being older than Joe, although her exact age is not specified.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We have an 'extract_entities' function that looks for simple name/age pairs.\n",
    "Let's see if the LLM picks that function for a user query describing people.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_3 = \"Joe is 30, Mary is older but we don't know her age.\"\n",
    "resp3 = test_call_model(user_prompt_3, function_call=\"auto\")\n",
    "print(\"\\n=== Entity Extraction Test ===\")\n",
    "print(\"Model response:\\n\", resp3)\n",
    "final_text_3 = handle_function_call(resp3)\n",
    "print(\"\\nUser-Facing Answer:\\n\", final_text_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 8) Section: Tagging\n",
    "# -------------------------------------------------\n",
    " Why This Is Useful:\n",
    " Tagging or classification can be critical for sentiment analysis,\n",
    " content moderation, or language detection. By letting the LLM call\n",
    " a specialized \"tag_text\" function, you can unify your external logic\n",
    " (like a custom sentiment model) with the language reasoning of the LLM.\n",
    "\n",
    " (Implementation of the function or usage example would go here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tagging Test ===\n",
      "Model response:\n",
      " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_7WXLDwLFeNxsdzMpB4podXQh', function=Function(arguments='{\"text\":\"nypd is better than joe\\'s pizza\"}', name='tag_text'), type='function')])\n",
      "Model wants to call function tag_text with args {'text': \"nypd is better than joe's pizza\"}\n",
      "\n",
      "User-Facing Answer:\n",
      " It appears that the statement \"nypd is better than joe's pizza\" has been classified as neutral in sentiment and is in English. What specific information or further analysis do you need regarding this text?\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We have a 'tag_text' function that returns a naive sentiment & language.\n",
    "We'll see if the model calls it automatically if the user requests tagging.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_4 = \"Can you tag this text for me: 'nypd is better than joe's pizza'\"\n",
    "resp4 = test_call_model(user_prompt_4, function_call=\"auto\")\n",
    "print(\"\\n=== Tagging Test ===\")\n",
    "print(\"Model response:\\n\", resp4)\n",
    "final_text_4 = handle_function_call(resp4)\n",
    "print(\"\\nUser-Facing Answer:\\n\", final_text_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 9) Activity / Optional Challenge\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1) Try combining the two tasks into one user request:\n",
    "   e.g. \"What's the weather in New York, \n",
    "         and also please book me a flight from LAX to SFO next Friday at 9am with United.\"\n",
    "\n",
    "   Observe if the model tries to call multiple functions or just one. \n",
    "   Because the standard function calling only returns ONE call at a time, \n",
    "   you might get partial coverage. \n",
    "   (HINT: you'll need to loop to handle multiple calls or do more advanced logic.)\n",
    "\n",
    "2) Create a third function, e.g. \"file_complaint(name, email, text)\", \n",
    "   to simulate a user wanting to file a complaint about their flight.\n",
    "   Add that to function_descriptions, \n",
    "   then see if the model picks it up when the user says \n",
    "   \"I want to file a complaint about my missed flight. My name is Jane, email is jane@example.com\"\n",
    "\n",
    "3) Experiment with forcing a function call:\n",
    "   - function_call=\"none\": The model won't produce any function calls.\n",
    "   - function_call={\"name\":\"book_flight\"}: The model *must* call the 'book_flight' function, \n",
    "     which might lead to it guessing arguments if the user didn't specify them.\n",
    "\n",
    "4) If you want an advanced challenge, \n",
    "   handle repeated calls automatically:\n",
    "   - If the model calls one function, you feed the result, \n",
    "     then it calls a second function, etc.\n",
    "   - This is sometimes referred to as a \"multi-step\" or \"agentic\" approach.\n",
    "\n",
    "Have fun and experiment!\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------\n",
    "# 10) End\n",
    "# -------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
